
model_file: "./pp_model.prototxt"


dataset {
    name: "test_dataset"
    type: "DummyDataset"
    top: "data"
    dummydataset_param {
        n: 20
    }
}
dataloader {
    name: "test_dataloader"
    batch_size: 2
    num_workers: 8
    shuffle: false
    dataset: "test_dataset"
    phase: TEST
}


dataset {
    name: "train_dataset"
    type: "DummyDataset"
    top: "data"
    top: "label"
    dummydataset_param {
        n: 100
    }
}
dataloader {
    name: "train_dataloader"
    batch_size: 2
    num_workers: 8
    shuffle: true
    dataset: "train_dataset"
    phase: TRAIN
}



optimizer {
    name: "optimizer"
    
    type: "SGD"
    lr: 0.1
    momentum: 0.3
    
    params_group {
        lr: 0.01
        momentum: 0.2
        params_inline: "params_1 = [p for n, p in model.named_parameters() if 'weight' in n]"
    }
    params_group {
        lr: 0.1
        params_inline: "params_2 = [p for n, p in model.named_parameters() if not 'weight' in n]"
    }
    
    # module_file: "./pp_optimizer.py.txt"
    
}

lr_scheduler {
    name: "lr_scheduler_1"
    type: "MultiStepLR"
    gamma: 0.1
    milestones: [10, 20]
    
    # module_inline: "lr_scheduler_1 = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[1, 2], gamma=0.1)"
}
    

device: "cpu"

# distributed {
#     backend: "nccl"
#     init_method: "env://"
#     # world_size: 4
# }


epoches: 3

snapshot: 10
log_dir: "./logs"
prefix: "pp_model"







# optimizer {
#     code:"\n"
#         "import torch \n"
#         "print('---optmizer-----') \n"
#         "def optimizer(model, lr): \n"
#         "    return torch.optim.SGD(model.parameters(), lr=lr) \n"
#         "\n"
# }


# model {
#     name: "model"
#     
#     module {
#       name: "resnet"
#       type: "ResNet"
#       bottom: "conv1"
#       top: "resnet1"
#       resnet_param {
#         layers: "34"
#       }
#     }
#      
# }
