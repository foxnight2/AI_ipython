
model {
    module {
        type: "Resnet18"
        resnet18_param {
            pretrained: True
        }
        bottom: "data"
        top: "pred"
    }
    
    module {
        type: "CrossEntropyLoss"
        bottom: "pred"
        bottom: "label"
        top: "loss"
        
        phase: TRAIN
    }
    
    input_names: ["data", "label"]
    output_names: ["pred", "loss"]
}


dataset {
    name: "test_dataset"
    type: "DummyDataset"
    top: "data"
    dummy_dataset_param {
        n: 30
    }
}

dataloader {
    name: "test_dataloader"
    batch_size: 5
    num_workers: 1
    shuffle: false
    dataset: "test_dataset"
    phase: EVAL
}


# dataset {
#     name: "train_dataset"
#     type: "CIFAR10"
#     top: "data"
#     top: "label"
#     cifar10_param {
#         root: "./"
#         train: true
#         download: false
#     }
# }

dataset {
    name: "train_dataset"
    type: "DummyDataset"
    top: "data"
    top: "label"
    dummy_dataset_param {
        n: 30
    }
}


dataloader {
    name: "train_dataloader"
    batch_size: 8
    num_workers: 2
    shuffle: true
    dataset: "train_dataset"
    phase: TRAIN
}


optimizer {
    name: "optimizer"
    
    type: "SGD"
    lr: 0.1
    momentum: 0.3
    
    params_group {
        lr: 0.01
        momentum: 0.2
        params_inline: "params_1 = [p for n, p in model.named_parameters() if 'weight' in n]"
    }
    params_group {
        lr: 0.1
        params_inline: "params_2 = [p for n, p in model.named_parameters() if not 'weight' in n]"
    }
    
    # module_file: "./pp_optimizer.py.txt"
    
}

lr_scheduler {
    name: "lr_scheduler_1"
    type: "MultiStepLR"
    gamma: 0.1
    milestones: [10, 20]    
    # module_inline: "lr_scheduler_1 = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[1, 2], gamma=0.1)"
}


device: "cuda:0"
use_amp: False

distributed {
    enabled: True
    backend: "nccl"
    init_method: "env://"
}

epoches: 2

snapshot: 10
log_dir: "./logs"
prefix: "pp_model"

