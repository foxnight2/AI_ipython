{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.cvlibs.net/publications/Geiger2013IJRR.pdf\n",
    "# http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d\n",
    "# https://github.com/joseph-zhong/KITTI-devkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    ".\n",
    "├── data_object_calib.zip\n",
    "├── data_object_image_2.zip\n",
    "├── data_object_label_2.zip\n",
    "├── data_object_velodyne.zip\n",
    "├── testing\n",
    "│   ├── calib\n",
    "│   │   ├── 000000.txt\n",
    "│   │   ├── 000001.txt\n",
    "│   │   ├── 007516.txt\n",
    "│   │   └── 007517.txt\n",
    "│   └── image_2\n",
    "│       ├── 000000.png\n",
    "│       ├── 000001.png\n",
    "│       └── 007517.png\n",
    "└── training\n",
    "    ├── calib\n",
    "    │   ├── 000000.txt\n",
    "    │   ├── 000001.txt\n",
    "    │   └── 007480.txt\n",
    "    ├── image_2\n",
    "    │   ├── 000000.png\n",
    "    │   ├── 000001.png\n",
    "    │   └── 007480.png\n",
    "    ├── label_2\n",
    "    │   ├── 000000.txt\n",
    "    │   ├── 000001.txt\n",
    "    │   └── 007480.txt\n",
    "    └── velodyne\n",
    "        ├── 000000.bin\n",
    "        ├── 000001.bin\n",
    "        └── 007480.bin\n",
    "\n",
    "8 directories, 44709 files\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pedestrian 0.00 0 -0.20 712.40 143.00 810.73 307.92 1.89 0.48 1.20 1.84 1.47 8.41 0.01\n",
    "\n",
    "# 712.40 143.00 810.73 307.92\n",
    "# 1.89 0.48 1.20\n",
    "# 1.84 1.47 8.41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "class KittiDetection3D(data.Dataset):\n",
    "    def __init__(self, root_dir='/root/paddlejob/workspace/env_run/workspace/dataset/kitti/', ):\n",
    "        '''KittiDetection3D\n",
    "        '''\n",
    "        train_root = os.path.join(root_dir, 'training') \n",
    "        annos_name = ['calib', 'image_2', 'label_2', 'velodyne']\n",
    "        \n",
    "        self.files = glob.glob(os.path.join())\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        pass\n",
    "    \n",
    "    def _proj_cam_im(self, ):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "KittiDetection3D()[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "###########################################################################\n",
    "#            THE KITTI VISION BENCHMARK SUITE: OBJECT BENCHMARK           #\n",
    "#              Andreas Geiger    Philip Lenz    Raquel Urtasun            #\n",
    "#                    Karlsruhe Institute of Technology                    #\n",
    "#                Toyota Technological Institute at Chicago                #\n",
    "#                             www.cvlibs.net                              #\n",
    "###########################################################################\n",
    "\n",
    "For recent updates see http://www.cvlibs.net/datasets/kitti/eval_object.php.\n",
    "\n",
    "This file describes the KITTI 2D object detection and orientation estimation\n",
    "benchmark, the 3D object detection benchmark and the bird's eye view benchmark.\n",
    "The benchmarks consist of 7481 training images (and point clouds) \n",
    "and 7518 test images (and point clouds) for each task.\n",
    "Despite the fact that we have labeled 8 different classes, only the\n",
    "classes 'Car' and 'Pedestrian' are evaluated in our benchmark, as only for\n",
    "those classes enough instances for a comprehensive evaluation have been\n",
    "labeled. The labeling process has been performed in two steps: First we\n",
    "hired a set of annotators, to label 3D bounding boxe tracklets in point\n",
    "clouds. Since for a pedestrian tracklet, a single 3D bounding box tracklet\n",
    "(dimensions have been fixed) often fits badly, we additionally labeled the\n",
    "left/right boundaries of each object by making use of Mechanical Turk. We\n",
    "also collected labels of the object's occlusion state, and computed the\n",
    "object's truncation via backprojecting a car/pedestrian model into the\n",
    "image plane.\n",
    "\n",
    "NOTE: WHEN SUBMITTING RESULTS, PLEASE STORE THEM IN THE SAME DATA FORMAT IN\n",
    "WHICH THE GROUND TRUTH DATA IS PROVIDED (SEE BELOW), USING THE FILE NAMES\n",
    "000000.txt 000001.txt ... CREATE A ZIP ARCHIVE OF THEM AND STORE YOUR\n",
    "RESULTS (ONLY THE RESULTS OF THE TEST SET) IN ITS ROOT FOLDER.\n",
    "\n",
    "NOTE2: Please read the bottom of this file carefully if you plan to evaluate\n",
    "results yourself on the training set.\n",
    "\n",
    "NOTE3: WHEN SUBMITTING RESULTS FOR THE 3D OBJECT DETECTION BENCHMARK OR THE\n",
    "BIRD'S EYE VIEW BENCHMARK (AS OF 2017), READ THE INSTRUCTIONS BELOW CAREFULLY.\n",
    "IN PARTICULAR, MAKE SURE TO ALWAYS SUBMIT BOTH THE 2D BOUNDING BOXES AND THE\n",
    "3D BOUNDING BOXES AND FILTER BOUNDING BOXES NOT VISIBLE ON THE IMAGE PLANE.\n",
    "\n",
    "Data Format Description\n",
    "=======================\n",
    "\n",
    "The data for training and testing can be found in the corresponding folders.\n",
    "The sub-folders are structured as follows:\n",
    "\n",
    "  - image_02/ contains the left color camera images (png)\n",
    "  - label_02/ contains the left color camera label files (plain text files)\n",
    "  - calib/ contains the calibration for all four cameras (plain text file)\n",
    "\n",
    "The label files contain the following information, which can be read and\n",
    "written using the matlab tools (readLabels.m, writeLabels.m) provided within\n",
    "this devkit. All values (numerical or strings) are separated via spaces,\n",
    "each row corresponds to one object. The 15 columns represent:\n",
    "\n",
    "#Values    Name      Description\n",
    "----------------------------------------------------------------------------\n",
    "   1    type         Describes the type of object: 'Car', 'Van', 'Truck',\n",
    "                     'Pedestrian', 'Person_sitting', 'Cyclist', 'Tram',\n",
    "                     'Misc' or 'DontCare'\n",
    "   1    truncated    Float from 0 (non-truncated) to 1 (truncated), where\n",
    "                     truncated refers to the object leaving image boundaries\n",
    "   1    occluded     Integer (0,1,2,3) indicating occlusion state:\n",
    "                     0 = fully visible, 1 = partly occluded\n",
    "                     2 = largely occluded, 3 = unknown\n",
    "   1    alpha        Observation angle of object, ranging [-pi..pi]\n",
    "   4    bbox         2D bounding box of object in the image (0-based index):\n",
    "                     contains left, top, right, bottom pixel coordinates\n",
    "   3    dimensions   3D object dimensions: height, width, length (in meters)\n",
    "   3    location     3D object location x,y,z in camera coordinates (in meters)\n",
    "   1    rotation_y   Rotation ry around Y-axis in camera coordinates [-pi..pi]\n",
    "   1    score        Only for results: Float, indicating confidence in\n",
    "                     detection, needed for p/r curves, higher is better.\n",
    "\n",
    "Here, 'DontCare' labels denote regions in which objects have not been labeled,\n",
    "for example because they have been too far away from the laser scanner. To\n",
    "prevent such objects from being counted as false positives our evaluation\n",
    "script will ignore objects detected in don't care regions of the test set.\n",
    "You can use the don't care labels in the training set to avoid that your object\n",
    "detector is harvesting hard negatives from those areas, in case you consider\n",
    "non-object regions from the training images as negative examples.\n",
    "\n",
    "The coordinates in the camera coordinate system can be projected in the image\n",
    "by using the 3x4 projection matrix in the calib folder, where for the left\n",
    "color camera for which the images are provided, P2 must be used. The\n",
    "difference between rotation_y and alpha is, that rotation_y is directly\n",
    "given in camera coordinates, while alpha also considers the vector from the\n",
    "camera center to the object center, to compute the relative orientation of\n",
    "the object with respect to the camera. For example, a car which is facing\n",
    "along the X-axis of the camera coordinate system corresponds to rotation_y=0,\n",
    "no matter where it is located in the X/Z plane (bird's eye view), while\n",
    "alpha is zero only, when this object is located along the Z-axis of the\n",
    "camera. When moving the car away from the Z-axis, the observation angle\n",
    "will change.\n",
    "\n",
    "To project a point from Velodyne coordinates into the left color image,\n",
    "you can use this formula: x = P2 * R0_rect * Tr_velo_to_cam * y\n",
    "For the right color image: x = P3 * R0_rect * Tr_velo_to_cam * y\n",
    "\n",
    "Note: All matrices are stored row-major, i.e., the first values correspond\n",
    "to the first row. R0_rect contains a 3x3 matrix which you need to extend to\n",
    "a 4x4 matrix by adding a 1 as the bottom-right element and 0's elsewhere.\n",
    "Tr_xxx is a 3x4 matrix (R|t), which you need to extend to a 4x4 matrix \n",
    "in the same way!\n",
    "\n",
    "Note, that while all this information is available for the training data,\n",
    "only the data which is actually needed for the particular benchmark must\n",
    "be provided to the evaluation server. However, all 15 values must be provided\n",
    "at all times, with the unused ones set to their default values (=invalid) as\n",
    "specified in writeLabels.m. Additionally a 16'th value must be provided\n",
    "with a floating value of the score for a particular detection, where higher\n",
    "indicates higher confidence in the detection. The range of your scores will\n",
    "be automatically determined by our evaluation server, you don't have to\n",
    "normalize it, but it should be roughly linear. If you use writeLabels.m for\n",
    "writing your results, this function will take care of storing all required\n",
    "data correctly.\n",
    "\n",
    "2D Object Detection Benchmark\n",
    "=============================\n",
    "\n",
    "The goal in the 2D object detection task is to train object detectors for the\n",
    "classes 'Car', 'Pedestrian', and 'Cyclist'. The object detectors must\n",
    "provide as output the 2D 0-based bounding box in the image using the format\n",
    "specified above, as well as a detection score, indicating the confidence\n",
    "in the detection. All other values must be set to their default values\n",
    "(=invalid), see above. One text file per image must be provided in a zip\n",
    "archive, where each file can contain many detections, depending on the \n",
    "number of objects per image. In our evaluation we only evaluate detections/\n",
    "objects larger than 25 pixel (height) in the image and do not count 'Van' as\n",
    "false positives for 'Car' or 'Sitting Person' as false positive for 'Pedestrian'\n",
    "due to their similarity in appearance. As evaluation criterion we follow\n",
    "PASCAL and require the intersection-over-union of bounding boxes to be\n",
    "larger than 50% for an object to be detected correctly.\n",
    "\n",
    "Object Orientation Estimation Benchmark\n",
    "=======================================\n",
    "\n",
    "This benchmark is similar as the previous one, except that you have to\n",
    "provide additionally the most likely relative object observation angle\n",
    "(=alpha) for each detection. As described in our paper, our score here\n",
    "considers both, the detection performance as well as the orientation\n",
    "estimation performance of the algorithm jointly.\n",
    "\n",
    "3D Object Detection Benchmark\n",
    "=============================\n",
    "\n",
    "The goal in the 3D object detection task is to train object detectors for\n",
    "the classes 'Car', 'Pedestrian', and 'Cyclist'. The object detectors\n",
    "must provide BOTH the 2D 0-based bounding box in the image as well as the 3D\n",
    "bounding box (in the format specified above, i.e. 3D dimensions and 3D locations)\n",
    "and the detection score/confidence. Note that the 2D bounding box should correspond\n",
    "to the projection of the 3D bounding box - this is required to filter objects\n",
    "larger than 25 pixel (height). We also note that not all objects in the point clouds\n",
    "have been labeled. To avoid false positives, detections not visible on the image plane\n",
    "should be filtered (the evaluation does not take care of this, see \n",
    "'cpp/evaluate_object.cpp'). Similar to the 2D object detection benchmark,\n",
    "we do not count 'Van' as false positives for 'Car' or 'Sitting Person'\n",
    "as false positive for 'Pedestrian'. Evaluation criterion follows the 2D\n",
    "object detection benchmark (using 3D bounding box overlap).\n",
    "\n",
    "Bird's Eye View Benchmark\n",
    "=========================\n",
    "\n",
    "The goal in the bird's eye view detection task is to train object detectors\n",
    "for the classes 'Car', 'Pedestrian', and 'Cyclist' where the detectors must provide\n",
    "BOTH the 2D 0-based bounding box in the image as well as the 3D bounding box\n",
    "in bird's eye view and the detection score/confidence. This means that the 3D\n",
    "bounding box does not have to include information on the height axis, i.e.\n",
    "the height of the bounding box and the bounding box location along the height axis.\n",
    "For example, when evaluating the bird's eye view benchmark only (without the\n",
    "3D object detection benchmark), the height of the bounding box can be set to\n",
    "a value equal to or smaller than zero. Similarly, the y-axis location of the\n",
    "bounding box can be set to -1000 (note that an arbitrary negative value will\n",
    "not work). As above, we note that the 2D bounding boxes are required to filter\n",
    "objects larger than 25 pixel (height) and that - to avoid false positives - detections\n",
    "not visible on the image plane should be filtered. As in all benchmarks, we do\n",
    "not count 'Van' as false positives for 'Car' or 'Sitting Person' as false positive\n",
    "for 'Pedestrian'. Evaluation criterion follows the above benchmarks using\n",
    "a bird's eye view bounding box overlap.\n",
    "\n",
    "Mapping to Raw Data\n",
    "===================\n",
    "\n",
    "Note that this section is additional to the benchmark, and not required for\n",
    "solving the object detection task.\n",
    "\n",
    "In order to allow the usage of the laser point clouds, gps data, the right\n",
    "camera image and the grayscale images for the TRAINING data as well, we\n",
    "provide the mapping of the training set to the raw data of the KITTI dataset.\n",
    "\n",
    "This information is saved in mapping/train_mapping.txt and train_rand.txt:\n",
    "\n",
    "train_rand.txt: Random permutation, assigning a unique index to each image\n",
    "from the object detection training set. The index is 1-based.\n",
    "\n",
    "train_mapping.txt: Maps each unique index (= 1-based line numbers) to a zip\n",
    "file of the KITTI raw data set files. Note that those files are split into\n",
    "several categories on the website!\n",
    "\n",
    "Example: Image 0 from the training set has index 7282 and maps to date\n",
    "2011_09_28, drive 106 and frame 48. Drives and frames are 0-based.\n",
    "\n",
    "Evaluation Protocol:\n",
    "====================\n",
    "\n",
    "For transparency we have included the KITTI evaluation code in the\n",
    "subfolder 'cpp' of this development kit. It can be compiled via:\n",
    "\n",
    "g++ -O3 -DNDEBUG -o evaluate_object evaluate_object.cpp\n",
    "\n",
    "or using CMake and the provided 'CMakeLists.txt'.\n",
    "\n",
    "IMPORTANT NOTE:\n",
    "\n",
    "This code will result in 41 values (41 recall discretization steps). Following the MonoDIS paper\n",
    "\n",
    "https://research.mapillary.com/img/publications/MonoDIS.pdf\n",
    "\n",
    "from 8.10.2019 we compute the average precision not like in the PASCAL VOC protocol, but as follows:\n",
    "\n",
    "sum = 0;\n",
    "for (i=1; i<=40; i++)\n",
    "  sum += vals[i];\n",
    "average = sum/40.0;\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
