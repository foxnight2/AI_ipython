
# paper reading

Paper | Commits | Related  
---|---|---
[Word2vec]()| distributionl representation; cbow, skip-gram | [How to Train word2vec](http://jalammar.github.io/illustrated-word2vec/)
[GloVe]() | global information, coocurrent maxtrix|
[fastText]() | oov problem, letter n-gram | 
[Character-Aware](https://arxiv.org/pdf/1508.06615.pdf) | character-based, char-embedding->cnn->pool | 
[ELMo]() | BiLM; contextual, deep, character-based; [embedding, hidden1, hidden2] | 
[Transformer](https://arxiv.org/pdf/1706.03762.pdf) | [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html#batches-and-masking) | 
[GPT]() | | [gpt-2]()
[BERT](https://arxiv.org/pdf/1810.04805.pdf) | autoencoder(AE) language model; masked-lm; [bert-research by mc](http://mccormickml.com/2019/11/11/bert-research-ep-1-key-concepts-and-sources/#31-input-representation--wordpiece-embeddings); pre/post-norm | [RoBERTa: A Robustly Optimized BERT](https://arxiv.org/pdf/1907.11692.pdf); [Extreme language model compression with optimal subwords and shared projections](https://arxiv.org/pdf/1909.11687.pdf)
[XLNet]() | autoregressive(AR) language model; Permutation Language Modeling; Two-Stream Self-Attention; [difference between bert and xlnet](https://towardsdatascience.com/what-is-xlnet-and-why-it-outperforms-bert-8d8fce710335); 
