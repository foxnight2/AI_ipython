
# Attention

- self-attention
    - constant path length between any two positions
    - gating/multiplicative interactions
    - trivial to parallel (per layer)
    - model self-similarity
    - relative attention provides expressive timing, equivariance, and extends naturally to graphs.
    - can replace sequential computation entirely?
    - attention is a weighted average
    - attention is permutation invariant
    - residual carry positional information to higher layers, among another information
    - relative attention, multihead-attention + convolution?


- image
    - non-local mean
    - the image transfomer
    - gan 
    - conditional image completion
    - relative positions translational equivariance