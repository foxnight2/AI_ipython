# Basic Mathmetics

ID | Name | Commits
---|---|--- 
00 | [Linear Algebra Gilbert Strang MIT 18.065]() | --

---

case|expression
---|---
discrete | $\sum$
continuous | $\int$
--- | $log\prod \Rightarrow \sum$


---
## Linear Algebra


---
## Calculus
- Integration

- Differentiation
    - x
 
- [matrixcalculus online](http://www.matrixcalculus.org/)

---
## Probability
- Event $x$
- Random Variable $X$
- Probability $p(x)$
- Probability distribution

---
## Statistics
- Expectation $E(X)$
- Variance $Var(X)$

Concept | Expression | Commits
---|---|---
expectation | -- | --
variance | -- | --


---
## Imformation Thoery

| Concetp | Expression |  Commits |
| --- | --- | --- | 
Information | $I(x)=-logp_x$| $x$ is Event; Low probabilty == Higt information, same otherwise; 
Entropy | $H(X)=-\sum_{x\subseteq X}p(x)logp(x)$ | Uncentainty
Conditional Entropy | $H(Y/X)=H(X,Y)-H(X)$ |
Relative Entropy/KL Divergence| $D_{KL}(p\|q)=H_p(q)-H(p)$ | --
Cross Entropy| $CEH(p,q)=E_p(-logq)=H(p)+D_{KL}(p\|q)$ | --
Mutual Information | $I\left ( X,Y \right )=H(Y)-H(Y/X)= \sum _{x\subseteq X}\sum_{y\subseteq Y}p\left ( x,y \right )log\frac{p(x,y)}{p(x)p(y)}$

- Entropy
    - 
    - $H(X)\geqslant0$
    - $H(X,Y)=H(X)+H(Y), if(X,Y),independent$
    - $H(X,Y)=H(X)+H(Y)-I(X,Y),elsewise$

---

